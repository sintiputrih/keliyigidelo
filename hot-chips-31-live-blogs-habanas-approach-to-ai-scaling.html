<!doctype html><html lang=en itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=robots content="index,follow,noarchive"><title>Habana's Approach to AI Scaling - JazzVib</title><meta name=description content="09:21PM EDT - The final talk today at Hot Chips is from Habana, who is discussing its approach to how to scale AI compute. 09:21PM EDT - Goya and Gaudi 09:22PM EDT - Recapping Training vs Inference requirements"><meta name=author content="Some Person"><script type=application/ld+json>{"@context":"http://schema.org","@type":"WebSite","name":"JazzVib","url":"\/"}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"Organization","name":"","url":"\/"}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"\/","name":"home"}},{"@type":"ListItem","position":3,"item":{"@id":"\/hot-chips-31-live-blogs-habanas-approach-to-ai-scaling.html","name":"Habana\u0027S approach to ai scaling"}}]}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"Article","author":{"name":"Jenniffer Sheldon"},"headline":"Habana\u0027s Approach to AI Scaling","description":"09:21PM EDT - The final talk today at Hot Chips is from Habana, who is discussing its approach to how to scale AI compute. 09:21PM EDT - Goya and Gaudi 09:22PM EDT - Recapping Training vs Inference requirements","inLanguage":"en","wordCount":669,"datePublished":"2024-06-07T00:00:00","dateModified":"2024-06-07T00:00:00","image":"\/img\/avatar-icon.png","keywords":[""],"mainEntityOfPage":"\/hot-chips-31-live-blogs-habanas-approach-to-ai-scaling.html","publisher":{"@type":"Organization","name":"\/","logo":{"@type":"ImageObject","url":"\/img\/avatar-icon.png","height":60,"width":60}}}</script><meta property="og:title" content="Habana's Approach to AI Scaling"><meta property="og:description" content="09:21PM EDT - The final talk today at Hot Chips is from Habana, who is discussing its approach to how to scale AI compute. 09:21PM EDT - Goya and Gaudi 09:22PM EDT - Recapping Training vs Inference requirements"><meta property="og:image" content="/img/avatar-icon.png"><meta property="og:url" content="/hot-chips-31-live-blogs-habanas-approach-to-ai-scaling.html"><meta property="og:type" content="website"><meta property="og:site_name" content="JazzVib"><meta name=twitter:title content="Habana's Approach to AI Scaling"><meta name=twitter:description content="09:21PM EDT - The final talk today at Hot Chips is from Habana, who is discussing its approach to how to scale AI compute. 09:21PM EDT - Goya and Gaudi 09:22PM EDT - Recapping Training vs Inference â€¦"><meta name=twitter:image content="/img/avatar-icon.png"><meta name=twitter:card content="summary"><meta name=twitter:site content="@username"><meta name=twitter:creator content="@username"><link href=./img/favicon.ico rel=icon type=image/x-icon><meta name=generator content="Hugo 0.98.0"><link rel=alternate href=./index.xml type=application/rss+xml title=JazzVib><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css integrity=sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.5.0/css/all.css integrity=sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU crossorigin=anonymous><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css integrity=sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u crossorigin=anonymous><link rel=stylesheet href=https://assets.cdnweb.info/hugo/bh/css/main.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic"><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800"><link rel=stylesheet href=https://assets.cdnweb.info/hugo/bh/css/highlight.min.css><link rel=stylesheet href=https://assets.cdnweb.info/hugo/bh/css/codeblock.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css integrity=sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css integrity=sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R crossorigin=anonymous></head><body><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header class=header-section><div class="intro-header no-img"><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><h1>Habana's Approach to AI Scaling</h1><span class=post-meta><i class="fas fa-calendar"></i>&nbsp;Posted on June 7, 2024
&nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;4&nbsp;minutes
&nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;669&nbsp;words
&nbsp;|&nbsp;<i class="fas fa-user"></i>&nbsp;Jenniffer Sheldon</span></div></div></div></div></div></header><div class=container role=main><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><article role=main class=blog-post><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_184445_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_184403_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819212118 href=#><span class=lb_time>09:21PM EDT</span></a> - The final talk today at Hot Chips is from Habana, who is discussing its approach to how to scale AI compute.</p><p><a id=post0819212122 href=#><span class=lb_time>09:21PM EDT</span></a> - Goya and Gaudi</p><p><a id=post0819212202 href=#><span class=lb_time>09:22PM EDT</span></a> - Recapping Training vs Inference requirements</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_182142_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819212411 href=#><span class=lb_time>09:24PM EDT</span></a> - Goya processor architecure</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_182354_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819212428 href=#><span class=lb_time>09:24PM EDT</span></a> - 3 engines, RPC, GEMM, and DMA. Work Concurrently with shared SRAM</p><p><a id=post0819212439 href=#><span class=lb_time>09:24PM EDT</span></a> - TPC is VLIW SIMD core, C-programmable</p><p><a id=post0819212445 href=#><span class=lb_time>09:24PM EDT</span></a> - PCIe Gen 4.0 x16</p><p><a id=post0819212457 href=#><span class=lb_time>09:24PM EDT</span></a> - Two DDR4-2666 channels, built on TSMC 16</p><p><a id=post0819212508 href=#><span class=lb_time>09:25PM EDT</span></a> - Supports UINT8 to FP32</p><p><a id=post0819212519 href=#><span class=lb_time>09:25PM EDT</span></a> - Dedicated HW and TPC ISA for special function acceneration</p><p><a id=post0819212555 href=#><span class=lb_time>09:25PM EDT</span></a> - Have to adjust quantization to mix accuracy vs power</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_182600_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819212646 href=#><span class=lb_time>09:26PM EDT</span></a> - PCIe card - Software stack is more important.</p><p><a id=post0819212658 href=#><span class=lb_time>09:26PM EDT</span></a> - Habana is a software company that just happens to do hardware</p><p><a id=post0819212719 href=#><span class=lb_time>09:27PM EDT</span></a> - Graph compiler with built-in quantization engine</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_182703_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819212730 href=#><span class=lb_time>09:27PM EDT</span></a> - Multiple recipes can be loaded for the hardware</p><p><a id=post0819212801 href=#><span class=lb_time>09:28PM EDT</span></a> - Goya supports models trained on any processor: CPU, GPU, TPU, Gaudi etc</p><p><a id=post0819212837 href=#><span class=lb_time>09:28PM EDT</span></a> - Users can create custom layers and kernels</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_182844_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819212918 href=#><span class=lb_time>09:29PM EDT</span></a> - Still market leader since benchmarks made 11 months ago vs common CPU/GPU</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_182931_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819212948 href=#><span class=lb_time>09:29PM EDT</span></a> - New for today, natural language benchmark results</p><p><a id=post0819213004 href=#><span class=lb_time>09:30PM EDT</span></a> - Support BERT architecture on Goya</p><p><a id=post0819213019 href=#><span class=lb_time>09:30PM EDT</span></a> - GEMMs and TPCs are fully utilized</p><p><a id=post0819213027 href=#><span class=lb_time>09:30PM EDT</span></a> - Chip was designed long before BERT was invested</p><p><a id=post0819213029 href=#><span class=lb_time>09:30PM EDT</span></a> - invented</p><p><a id=post0819213039 href=#><span class=lb_time>09:30PM EDT</span></a> - High degree of accuracy when quantized</p><p><a id=post0819213044 href=#><span class=lb_time>09:30PM EDT</span></a> - Software managed SRAM</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_183054_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819213135 href=#><span class=lb_time>09:31PM EDT</span></a> - Now Gaudi, the training processor</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_183121_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819213150 href=#><span class=lb_time>09:31PM EDT</span></a> - Performance at Scale, high throughput at low batch size, high power efficiency</p><p><a id=post0819213207 href=#><span class=lb_time>09:32PM EDT</span></a> - Enable native ethernet scale out - on chip RDMA over Converged Ethernet</p><p><a id=post0819213225 href=#><span class=lb_time>09:32PM EDT</span></a> - Open Compute Project Accelerator Module: OAM = (OCP)AM</p><p><a id=post0819213237 href=#><span class=lb_time>09:32PM EDT</span></a> - Framework and ML compiler support, rich TPC Kernet Library</p><p><a id=post0819213257 href=#><span class=lb_time>09:32PM EDT</span></a> - Architecture looks similar to Goya</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_183241_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819213305 href=#><span class=lb_time>09:33PM EDT</span></a> - Networking has changed, memory has changed</p><p><a id=post0819213318 href=#><span class=lb_time>09:33PM EDT</span></a> - PCIe 4.0 x16, 4x8GB HBM</p><p><a id=post0819213325 href=#><span class=lb_time>09:33PM EDT</span></a> - 10x 100 GbE, or 20x50 GbE</p><p><a id=post0819213340 href=#><span class=lb_time>09:33PM EDT</span></a> - Supports UINT8 to FP32 and BF16</p><p><a id=post0819213417 href=#><span class=lb_time>09:34PM EDT</span></a> - SW supports profiling tools</p><p><a id=post0819213447 href=#><span class=lb_time>09:34PM EDT</span></a> - Only AI Training chip with RoCE v2</p><p><a id=post0819213509 href=#><span class=lb_time>09:35PM EDT</span></a> - NVIDIA was first to showcase RoCE v2 for AI, but they haven't implemented it yet</p><p><a id=post0819213608 href=#><span class=lb_time>09:36PM EDT</span></a> - NVIDIA GPU is much more complex with RoCE v2 support via Mellanox</p><p><a id=post0819213614 href=#><span class=lb_time>09:36PM EDT</span></a> - Gaudi integrates both</p><p><a id=post0819213632 href=#><span class=lb_time>09:36PM EDT</span></a> - Supports Lossless and Lossy fabrics</p><p><a id=post0819213641 href=#><span class=lb_time>09:36PM EDT</span></a> - Advanced congestion controls</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_183432_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_183525_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_183548_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_183715_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819213740 href=#><span class=lb_time>09:37PM EDT</span></a> - Customers can buy OAM cards or an 8 card Server</p><p><a id=post0819213816 href=#><span class=lb_time>09:38PM EDT</span></a> - Server box has no CPU, up to customer to config to needed. Uses mini-SAS HD</p><p><a id=post0819213831 href=#><span class=lb_time>09:38PM EDT</span></a> - Ethernet connectivity for point-to-point links with non-blocking full mesh</p><p><a id=post0819213842 href=#><span class=lb_time>09:38PM EDT</span></a> - 3 ports per card for scale up</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_183747_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819213905 href=#><span class=lb_time>09:39PM EDT</span></a> - Can choose ratio of CPUs to Gaudi cards</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_183849_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819213931 href=#><span class=lb_time>09:39PM EDT</span></a> - Gaudi vs DGX</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_183918_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819214037 href=#><span class=lb_time>09:40PM EDT</span></a> - Unlike DGX, do not force user to separate PCIe between management and scaleout. Gaudi offers separate PCIe ports</p><p><a id=post0819214103 href=#><span class=lb_time>09:41PM EDT</span></a> - PCIe card dual slot also available</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_184043_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819214105 href=#><span class=lb_time>09:41PM EDT</span></a> - HL-200</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/14760/IMG_20190819_184117_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0819214133 href=#><span class=lb_time>09:41PM EDT</span></a> - Data parallel possible, model parallel possible</p><p><a id=post0819214440 href=#><span class=lb_time>09:44PM EDT</span></a> - Can leapfrog performance over DGX-2 due to better connectivity. Can connect 64 gaudi chips with non-blocking throughput</p><p><a id=post0819214538 href=#><span class=lb_time>09:45PM EDT</span></a> - Q&A time</p><p><a id=post0819214645 href=#><span class=lb_time>09:46PM EDT</span></a> - Q: What type of quantization requires a processor? There is no quantization processor. There's a software engine that takes an FP32 model and can quantize to data types that are more efficient and gives the feedback on the accuracy</p><p><a id=post0819214740 href=#><span class=lb_time>09:47PM EDT</span></a> - Q: Can you comment on interconnectivity of GEMM? A: It's one functional unit.</p><p><a id=post0819214818 href=#><span class=lb_time>09:48PM EDT</span></a> - Q: What is the minimum viable for an IoT gateway? A: You can use a single card. You can put a gaudi in a single PCIe slot.</p><p><a id=post0819214833 href=#><span class=lb_time>09:48PM EDT</span></a> - That's a wrap for today. More talks tomorrow!</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH51g5VpZqGnpGKwqbXPrGRsaV2htrexjJujqJ%2BjYrWirsCnmKxlkaW9s7vAnJ9mrJ9irqp50pyYpaGenA%3D%3D</p><hr><section id=social-share><div class="list-inline footer-links"><div class=share-box aria-hidden=true><ul class=share><li><a href="//twitter.com/share?url=%2fhot-chips-31-live-blogs-habanas-approach-to-ai-scaling.html&text=Habana%27s%20Approach%20to%20AI%20Scaling&via=username" target=_blank title="Share on Twitter"><i class="fab fa-twitter"></i></a></li><li><a href="//www.facebook.com/sharer/sharer.php?u=%2fhot-chips-31-live-blogs-habanas-approach-to-ai-scaling.html" target=_blank title="Share on Facebook"><i class="fab fa-facebook"></i></a></li><li><a href="//reddit.com/submit?url=%2fhot-chips-31-live-blogs-habanas-approach-to-ai-scaling.html&title=Habana%27s%20Approach%20to%20AI%20Scaling" target=_blank title="Share on Reddit"><i class="fab fa-reddit"></i></a></li><li><a href="//www.linkedin.com/shareArticle?url=%2fhot-chips-31-live-blogs-habanas-approach-to-ai-scaling.html&title=Habana%27s%20Approach%20to%20AI%20Scaling" target=_blank title="Share on LinkedIn"><i class="fab fa-linkedin"></i></a></li><li><a href="//www.stumbleupon.com/submit?url=%2fhot-chips-31-live-blogs-habanas-approach-to-ai-scaling.html&title=Habana%27s%20Approach%20to%20AI%20Scaling" target=_blank title="Share on StumbleUpon"><i class="fab fa-stumbleupon"></i></a></li><li><a href="//www.pinterest.com/pin/create/button/?url=%2fhot-chips-31-live-blogs-habanas-approach-to-ai-scaling.html&description=Habana%27s%20Approach%20to%20AI%20Scaling" target=_blank title="Share on Pinterest"><i class="fab fa-pinterest"></i></a></li></ul></div></div></section></article><ul class="pager blog-pager"><li class=previous><a href=./hitman-holla.html data-toggle=tooltip data-placement=top title="Hitman Holla">&larr; Previous Post</a></li><li class=next><a href=./all-about-cyndi-lauper-net-worth-career-bio.html data-toggle=tooltip data-placement=top title="All About Cyndi Lauper | Net Worth, Career, Bio">Next Post &rarr;</a></li></ul></div></div></div><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center footer-links"><li><a href title=RSS><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="credits copyright text-muted">All rights reserved
&nbsp;&bull;&nbsp;&copy;
2024
&nbsp;&bull;&nbsp;
<a href=./>JazzVib</a></p><p class="credits theme-by text-muted"><a href=https://gohugo.io>Hugo v0.98.0</a> powered &nbsp;&bull;&nbsp; Theme <a href=https://github.com/halogenica/beautifulhugo>Beautiful Hugo</a> adapted from <a href=https://deanattali.com/beautiful-jekyll/>Beautiful Jekyll</a></p></div></div></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js integrity=sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js integrity=sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe crossorigin=anonymous></script>
<script src=https://code.jquery.com/jquery-1.12.4.min.js integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin=anonymous></script>
<script src=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js integrity=sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa crossorigin=anonymous></script>
<script src=https://assets.cdnweb.info/hugo/bh/js/main.js></script>
<script src=https://assets.cdnweb.info/hugo/bh/js/highlight.min.js></script>
<script>hljs.initHighlightingOnLoad()</script><script>$(document).ready(function(){$("pre.chroma").css("padding","0")})</script><script>renderMathInElement(document.body)</script><script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js integrity=sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js integrity=sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q crossorigin=anonymous></script><script src=https://assets.cdnweb.info/hugo/bh/js/load-photoswipe.js></script>
<script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/banner.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>